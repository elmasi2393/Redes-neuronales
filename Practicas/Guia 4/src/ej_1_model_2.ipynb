{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciones de activaciÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = lambda h: np.tanh(h)\n",
    "derivate_act = lambda h: 1 - np.tanh(h)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entradas de la compuerta\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "# salidas\n",
    "Y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Semilla aleatoria para reproducibilidad\n",
    "np.random.seed(42)\n",
    "\n",
    "hidden_dim = 1\n",
    "input_dim = 2\n",
    "\n",
    "# Pesos entre capa de entrada y capa oculta (2x1)\n",
    "weights_input_hidden = np.random.rand(input_dim, hidden_dim) \n",
    "# Pesos entre capa oculta y capa de salida (3x1)\n",
    "weights_hidden_output = np.random.rand(hidden_dim + input_dim, 1)\n",
    "\n",
    "b0 = np.random.rand(hidden_dim)\n",
    "b1 = np.random.rand(1)\n",
    "\n",
    "# Definimos la tasa de aprendizaje\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN(x, w_ih, w_ho, b1, b0):#+\n",
    "    # Vj = np.zeros((X.shape[0], w_ih.shape[1]))\n",
    "    oi = np.zeros((X.shape[0], w_ho.shape[1]))\n",
    "\n",
    "    for mu in range(X.shape[0]):    # itero en todos los ejemplos\n",
    "        # input -> hidden\n",
    "        Vj = activation(np.dot(X[mu], w_ih) + b0)\n",
    "        # print(Vj[mu].shape, x[mu].shape)\n",
    "        # concateno la salida de la oculta con las entradas\n",
    "        concatenated_input = np.concatenate((x[mu], Vj))\n",
    "        # hidden -> output\n",
    "        oi[mu] = activation(np.dot(concatenated_input, w_ho) + b1)\n",
    "    \n",
    "    return oi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10000\n",
    "\n",
    "\n",
    "for i in range(epochs):\n",
    "    # Initialize gradients\n",
    "    grad_w_0_1 = np.zeros_like(weights_input_hidden)\n",
    "    grad_w_1_2 = np.zeros_like(weights_hidden_output)\n",
    "    grad_b0 = np.zeros_like(b0)\n",
    "    grad_b1 = np.zeros_like(b1)\n",
    "\n",
    "    # Forward projection for all examples\n",
    "    hj = np.dot(X, weights_input_hidden) + b0  # Shape: (num_samples, hidden_dim)\n",
    "    Vj = activation(hj)  # Apply activation function element-wise\n",
    "\n",
    "    concatenate_input = np.concatenate((X, Vj), axis=1)  # Shape: (num_samples, input_dim + hidden_dim)\n",
    "\n",
    "    hi = np.dot(concatenate_input, weights_hidden_output) + b1  # Shape: (num_samples, output_dim)\n",
    "    oi = activation(hi)  # Apply activation function element-wise\n",
    "\n",
    "    # Backpropagation\n",
    "    # hidden -> output\n",
    "    delta_i_mu = derivate_act(hi) * (Y - oi)  # Shape: (num_samples, output_dim)\n",
    "    grad_w_1_2 += np.dot(concatenate_input.T, delta_i_mu)  # Update weights (input_dim + hidden_dim, output_dim)\n",
    "    grad_b1 += delta_i_mu.sum(axis=0)  # Sum over the samples\n",
    "\n",
    "    # input -> hidden\n",
    "    delta_j_mu = derivate_act(hj) * np.dot(delta_i_mu, weights_hidden_output[input_dim:].T)  # Shape: (num_samples, hidden_dim)\n",
    "    grad_w_0_1 += np.dot(X.T, delta_j_mu)  # Update weights (input_dim, hidden_dim)\n",
    "    grad_b0 += delta_j_mu.sum(axis=0)  # Sum over the samples\n",
    "\n",
    "    # Actualizo los pesos y los bias\n",
    "    weights_input_hidden += learning_rate * grad_w_0_1\n",
    "    weights_hidden_output += learning_rate * grad_w_1_2\n",
    "    b0 += learning_rate * grad_b0\n",
    "    b1 += learning_rate * grad_b1\n",
    "\n",
    "    # # Reset gradients for the next epoch\n",
    "    # grad_w_0_1.fill(0)\n",
    "    # grad_w_1_2.fill(0)\n",
    "    # grad_b0.fill(0)\n",
    "    # grad_b1.fill(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "\n",
    "for i in range(epochs):\n",
    "    grad_w_0_1 = np.zeros_like(weights_input_hidden)\n",
    "    grad_w_1_2 = np.zeros_like(weights_hidden_output)\n",
    "    grad_b0 = np.zeros_like(b0)\n",
    "    grad_b1 = np.zeros_like(b1)\n",
    "    \n",
    "    for mu, x in enumerate(X):  # Itero en los ejemplos\n",
    "        # Forward projection\n",
    "        hj = np.dot(x, weights_input_hidden) + b0\n",
    "        Vj = activation(hj)\n",
    "\n",
    "        concatenate_input = np.concatenate((x, Vj))\n",
    "\n",
    "        # print('concatenate_input shape ', concatenate_input.shape)\n",
    "\n",
    "        hi = np.dot(concatenate_input, weights_hidden_output) + b1\n",
    "        oi = activation(hi)\n",
    "\n",
    "        # Backpropagation\n",
    "        # hidden -> output\n",
    "        delta_i_mu = derivate_act(hi) * (Y[mu] - oi)\n",
    "        grad_w_1_2 += np.outer(concatenate_input, delta_i_mu)\n",
    "        grad_b1 += delta_i_mu.flatten()\n",
    "\n",
    "        # input -> hidden\n",
    "        delta_j_mu = derivate_act(hj) * np.dot(weights_hidden_output[input_dim:], delta_i_mu)\n",
    "        grad_w_0_1 += np.outer(x, delta_j_mu)\n",
    "        grad_b0 += delta_j_mu.flatten()\n",
    "\n",
    "    # Actualizo los pesos y los bias\n",
    "    weights_input_hidden += learning_rate * grad_w_0_1\n",
    "    weights_hidden_output += learning_rate * grad_w_1_2\n",
    "    b0 += learning_rate * grad_b0\n",
    "    b1 += learning_rate * grad_b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "\n",
    "\n",
    "# Function to perform training with mini-batch gradient descent\n",
    "def train_model(X, Y, weights_input_hidden, weights_hidden_output, b0, b1, learning_rate, epochs, batch_size=32):\n",
    "    num_samples = X.shape[0]  # Total number of samples\n",
    "\n",
    "    for i in range(epochs):\n",
    "        # Shuffle data at the beginning of each epoch\n",
    "        indices = np.random.permutation(num_samples)\n",
    "        x_shuffled = X[indices]\n",
    "        y_shuffled = Y[indices]\n",
    "\n",
    "        for start in range(0, num_samples, batch_size):\n",
    "            end = min(start + batch_size, num_samples)\n",
    "            X_batch = x_shuffled[start:end]\n",
    "            Y_batch = y_shuffled[start:end]\n",
    "\n",
    "            # Initialize gradients\n",
    "            grad_w_0_1 = np.zeros_like(weights_input_hidden)\n",
    "            grad_w_1_2 = np.zeros_like(weights_hidden_output)\n",
    "            grad_b0 = np.zeros_like(b0)\n",
    "            grad_b1 = np.zeros_like(b1)\n",
    "\n",
    "            # Forward projection for the mini-batch\n",
    "            hj = np.dot(X_batch, weights_input_hidden) + b0  # Shape: (batch_size, hidden_dim)\n",
    "            Vj = activation(hj)  # Apply activation function element-wise\n",
    "\n",
    "            concatenate_input = np.concatenate((X_batch, Vj), axis=1)  # Shape: (batch_size, input_dim + hidden_dim)\n",
    "\n",
    "            hi = np.dot(concatenate_input, weights_hidden_output) + b1  # Shape: (batch_size, output_dim)\n",
    "            oi = activation(hi)  # Apply activation function element-wise\n",
    "\n",
    "            # Backpropagation\n",
    "            # hidden -> output\n",
    "            delta_i_mu = derivate_act(hi) * (Y_batch - oi)  # Shape: (batch_size, output_dim)\n",
    "            grad_w_1_2 += np.dot(concatenate_input.T, delta_i_mu)  # Update weights (input_dim + hidden_dim, output_dim)\n",
    "            grad_b1 += delta_i_mu.sum(axis=0)  # Sum over the batch\n",
    "\n",
    "            # input -> hidden\n",
    "            delta_j_mu = derivate_act(hj) * np.dot(delta_i_mu, weights_hidden_output[X_batch.shape[1]:].T)  # Shape: (batch_size, hidden_dim)\n",
    "            grad_w_0_1 += np.dot(X_batch.T, delta_j_mu)  # Update weights (input_dim, hidden_dim)\n",
    "            grad_b0 += delta_j_mu.sum(axis=0)  # Sum over the batch\n",
    "\n",
    "            # Actualizo los pesos y los bias\n",
    "            weights_input_hidden += learning_rate * grad_w_0_1\n",
    "            weights_hidden_output += learning_rate * grad_w_1_2\n",
    "            b0 += learning_rate * grad_b0\n",
    "            b1 += learning_rate * grad_b1\n",
    "\n",
    "# Usage\n",
    "train_model(X, Y, weights_input_hidden, weights_hidden_output, b0, b1, learning_rate, epochs, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00238793]\n",
      " [0.92260247]\n",
      " [0.9228231 ]\n",
      " [0.01504687]]\n"
     ]
    }
   ],
   "source": [
    "print(NN(X, weights_input_hidden, weights_hidden_output, b1, b0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "repo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
